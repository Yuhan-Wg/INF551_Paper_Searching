{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Information \n",
    "\n",
    "Database Name: Paper_metadata.json\n",
    "\n",
    "Data Source: sciencemag.org\n",
    "\n",
    "Metadata: Title, Publication Year, Author, Category, Institution, DOI, Subject, Content:( Abstract, Citation, Full Content)\n",
    "\n",
    "### JSON Structure\n",
    "\n",
    "{\"maximumIndex\":metadata, \"data\":{content}}\n",
    "\n",
    "content(dictionary, keys are shown below):\n",
    "\n",
    "{'type',\n",
    "'title',\n",
    "'author',\n",
    "'authorInstitution',\n",
    "'publisher',\n",
    "'date',\n",
    "'doi',\n",
    "'pdfLink',\n",
    "'textLink',\n",
    "'text',\n",
    "'abstract',\n",
    "'discussion'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webpage Structure\n",
    "\n",
    "The archives in sicencemag/conten/by/year\n",
    "\n",
    "We can find search paper by year and week\n",
    "\n",
    "<img src='../pic/archives_by_year.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Click one we can get paper links\n",
    "\n",
    "<img src='../pic/contents_by_week.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Then Analyze the links we can get some metadata and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_url='http://science.sciencemag.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# too many urls here, scrap the urls first\n",
    "def scrapingURL(range1=2010,range2=2017):\n",
    "    #########################################\n",
    "    #\n",
    "    # Func scrapingUPL will return urlList\n",
    "    # urlList:{year:{week:{}}}\n",
    "    #\n",
    "    #########################################\n",
    "    \n",
    "    global basic_url\n",
    "    urlList = {}\n",
    "    #by years\n",
    "    for year in range(range1,range2+1):\n",
    "        #url address\n",
    "        html=basic_url+'/content/by/year/'+str(year)\n",
    "        \n",
    "        #directory[year]\n",
    "        urlList[year]={}\n",
    "        \n",
    "        #open the html\n",
    "        b0 = urlopen(html)\n",
    "        bs0bj = BeautifulSoup(b0,'html5lib')\n",
    "        \n",
    "        #url list\n",
    "        list = bs0bj.find('div',{'class':'pane-content'})\\\n",
    "        .findAll('a',{'class':'highwire-cite-linked-title'})\n",
    "        \n",
    "        #\n",
    "        for i in range(len(list)):\n",
    "            #weekly lests\n",
    "            urlList[year][list[i].get_text()[0:6]]=[]\n",
    "            \n",
    "            link=list[i].get('href')\n",
    "            b1 = urlopen(basic_url + link)\n",
    "            bs1bj = BeautifulSoup(b1, 'html5lib')\n",
    "            \n",
    "            #paper links\n",
    "            fullList = bs1bj.findAll('a', {'title': 'Full Text'})\n",
    "            \n",
    "            #store links\n",
    "            for content in fullList:\n",
    "                textUrl = content.get('href')\n",
    "                urlList[year][list[i].get_text()[0:6]].append(textUrl)\n",
    "            \n",
    "            #sleep\n",
    "            time.sleep(2 * (1+random.random()) )\n",
    "            print(\"finished: \"+str(year)+' '+list[i].get_text()[0:6])\n",
    "            return urlList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: 2017 Jan 06\n"
     ]
    }
   ],
   "source": [
    "urls = scrapingURL(range1=2017,range2=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scraping(basic_url,urlList,year,typeList=['Editorial','Research Article','Review Article','Letter','Book Review']):\n",
    "    #text data\n",
    "    textList = {}\n",
    "    for week in urlList[year]:\n",
    "        #classified by weeks\n",
    "        textList[week] = []\n",
    "        for content in urlList[year][week]:\n",
    "            #read url\n",
    "            b1 = urlopen(basic_url + content)\n",
    "            bs1bj = BeautifulSoup(b1, 'html5lib')\n",
    "            # check if there is article categories\n",
    "            # Since there are many redundant information\n",
    "            try:\n",
    "                articleType = bs1bj.find('meta', {'name': 'citation_article_type'}).get('content')\n",
    "            except:\n",
    "                articleType = 'none'\n",
    "            # while the type of paper is in the list( We only scrap several specific types of articles)\n",
    "            # 'Editorial','Research Article','Review Article','Letter','Book Review'\n",
    "            if articleType in typeList:\n",
    "                \n",
    "                textList[week].append(analyzeWebpage(bs1bj))             \n",
    "                time.sleep(2 * (1+random.random()) )\n",
    "        print('finished:' + week)\n",
    "    return textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyzeWebpage(bs1bj):\n",
    "    #title\n",
    "    title = bs1bj.find('meta', {'name': 'citation_title'}).get('content')\n",
    "\n",
    "    #Author\n",
    "    authorList = bs1bj.findAll('meta', {'name': 'citation_author'})\n",
    "    author = []\n",
    "    for m in authorList:\n",
    "        author.append(m.get('content'))\n",
    "    \n",
    "    #Article type\n",
    "    articleType = bs1bj.find('meta', {'name': 'citation_article_type'}).get('content')\n",
    "\n",
    "    #Institution\n",
    "    institutionList = bs1bj.findAll('meta', {'name': 'citation_author_institution'})\n",
    "    institution = []\n",
    "    for InstiData in institutionList:\n",
    "        institution.append(InstiData.get('content'))\n",
    "\n",
    "    #Publisher\n",
    "    publisher = bs1bj.find('meta', {'name': 'citation_publisher'}).get('content')\n",
    "\n",
    "    #Publication data\n",
    "    pubDate = bs1bj.find('meta', {'name': 'citation_publication_date'}).get('content')\n",
    "\n",
    "    #DOI\n",
    "    doi = bs1bj.find('meta', {'name': 'citation_doi'}).get('content')\n",
    "\n",
    "    ##PDF (need to be verified)\n",
    "    pdfLink = bs1bj.find('link', {'title': 'Full Text (PDF)'}).get('href')\n",
    "\n",
    "    ##text (need to be verified)\n",
    "    textLink = bs1bj.find('link', {'title': 'Full Text (Plain)'}).get('href')\n",
    "\n",
    "    # put them in to directory\n",
    "    textDict = {}\n",
    "    textDict['type'] = articleType\n",
    "    textDict['title'] = title\n",
    "    textDict['author'] = author\n",
    "    textDict['authorInstitution'] = institution\n",
    "    textDict['publisher'] = publisher\n",
    "    textDict['date'] = pubDate\n",
    "    textDict['doi'] = doi\n",
    "    textDict['pdfLink'] = pdfLink\n",
    "    textDict['textLink'] = textLink\n",
    "    # content\n",
    "    textDict['text'] = ''\n",
    "    textDict['abstract'] = ''\n",
    "    textDict['introduction'] = ''\n",
    "    textDict['discussion'] = ''\n",
    "\n",
    "    # Data for function to search by abstrace, introduction, discussion part\n",
    "    textParts = bs1bj.findAll('p', {'id': re.compile('p\\-[0-9]*')})\n",
    "    for finding in textParts:\n",
    "        textDict['text'] += finding.get_text().replace('\\n',' ') + ' '\n",
    "    # Abstract\n",
    "    # use try/except since articles may not have abstract\n",
    "    try:\n",
    "        abstractParts = bs1bj.findAll('div', {'class': 'section abstract'})\n",
    "        for finding in abstractParts:\n",
    "            textDict['abstract'] += finding.get_text().replace('\\n',' ') + ' '\n",
    "    except:\n",
    "        textDict['abstract'] = None\n",
    "\n",
    "    # Introduction\n",
    "    try:\n",
    "        introParts = bs1bj.findAll('div', {'class': 'section introduction'})\n",
    "        for finding in introParts:\n",
    "            textDict['introduction'] +=finding.get_text().replace('\\n',' ') + ' '\n",
    "    except:\n",
    "        textDict['introduction'] = None\n",
    "\n",
    "    # Discussion\n",
    "    try:\n",
    "        discussionParts = bs1bj.findAll('div', {'class': 'section discussion'})\n",
    "        for finding in discussionParts:\n",
    "            textDict['discussion'] +=  finding.get_text().replace('\\n',' ') + ' '\n",
    "    except:\n",
    "        textDict['discussion'] = None\n",
    "\n",
    "    return textDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveasJSON(textList,year):\n",
    "    model= textList #数据\n",
    "    with open(\"../data/json/test.json\",'w',encoding='utf-8') as json_file:\n",
    "        maximumIndex = json_file['maximumIndex']\n",
    "        \n",
    "    with open(\"../data/json/test/data.json\",'w',encoding='utf-8') as json_file:\n",
    "        maximumIndex = json_file['maximumIndex']\n",
    "        for week in model:\n",
    "            jsonDataList = model[week]\n",
    "            for d in jsonDataList:\n",
    "                d\n",
    "                json.dump(d,json_file,ensure_ascii=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished:Jan 06\n"
     ]
    }
   ],
   "source": [
    "textList = scraping(basic_url,urls,2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model= {'maximumIndex':0, 'data':{}} #数据\n",
    "with open(\"../data/json/test.json\",'w',encoding='utf-8') as json_file:\n",
    "    json.dump(model,json_file,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#textList['Jan 06'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p= {'maximumIndex':-1, 'data':{}} #数据\n",
    "\n",
    "\n",
    "Index = p['maximumIndex']\n",
    "for week in textList:\n",
    "    for d in textList[week]:\n",
    "        Index += 1\n",
    "        p['data'][Index] = d\n",
    "p['maximumIndex'] = Index\n",
    "\n",
    "with open(\"../data/json/test.json\",'w',encoding='utf-8') as json_file:\n",
    "    json.dump(p,json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
