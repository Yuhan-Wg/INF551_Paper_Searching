{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Information \n",
    "\n",
    "Database Name: Paper_metadata.json\n",
    "\n",
    "Data Source: sciencemag.org\n",
    "\n",
    "Metadata: Title, Publication Year, Author, Category, Institution, DOI, Subject, Content:( Abstract, Citation, Full Content)\n",
    "\n",
    "### JSON Structure\n",
    "\n",
    "{\"maximumIndex\":metadata, \"data\":{content}}\n",
    "\n",
    "content(dictionary, keys are shown below):\n",
    "\n",
    "{'type',\n",
    "'title',\n",
    "'author',\n",
    "'authorInstitution',\n",
    "'publisher',\n",
    "'date',\n",
    "'doi',\n",
    "'pdfLink',\n",
    "'textLink',\n",
    "'text',\n",
    "'abstract',\n",
    "'discussion'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webpage Structure\n",
    "\n",
    "The archives in sicencemag/conten/by/year\n",
    "\n",
    "We can find search paper by year and week\n",
    "\n",
    "<img src='../pic/archives_by_year.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Click one we can get paper links\n",
    "\n",
    "<img src='../pic/contents_by_week.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Then Analyze the links we can get some metadata and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_url='http://science.sciencemag.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# too many urls here, scrap the urls first\n",
    "def scrapingURL(range1=2010,range2=2017):\n",
    "    #########################################\n",
    "    #\n",
    "    # Func scrapingUPL will return urlList\n",
    "    # urlList:{year:{week:{}}}\n",
    "    #\n",
    "    #########################################\n",
    "    \n",
    "    global basic_url\n",
    "    urlList = {}\n",
    "    #by years\n",
    "    for year in range(range1,range2+1):\n",
    "        #url address\n",
    "        html=basic_url+'/content/by/year/'+str(year)\n",
    "        \n",
    "        #directory[year]\n",
    "        urlList[year]={}\n",
    "        \n",
    "        #open the html\n",
    "        b0 = urlopen(html)\n",
    "        bs0bj = BeautifulSoup(b0,'html5lib')\n",
    "        \n",
    "        #url list\n",
    "        list_ = bs0bj.find('div',{'class':'pane-content'})\\\n",
    "        .findAll('a',{'class':'highwire-cite-linked-title'})\n",
    "        \n",
    "        #\n",
    "        for i in range(len(list_)):\n",
    "            #weekly lests\n",
    "            urlList[year][list_[i].get_text()[0:6]]=[]\n",
    "            \n",
    "            link=list_[i].get('href')\n",
    "            b1 = urlopen(basic_url + link)\n",
    "            bs1bj = BeautifulSoup(b1, 'html5lib')\n",
    "            \n",
    "            #paper links\n",
    "            fullList = bs1bj.findAll('a', {'title': 'Full Text'})\n",
    "            \n",
    "            #store links\n",
    "            for content in fullList:\n",
    "                textUrl = content.get('href')\n",
    "                urlList[year][list_[i].get_text()[0:6]].append(textUrl)\n",
    "            \n",
    "            #sleep\n",
    "            time.sleep(5 * (1+random.random()) )\n",
    "            print(\"finished: \"+str(year)+' '+list_[i].get_text()[0:6])\n",
    "    return urlList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: 2016 Jan 01\n",
      "finished: 2016 Jan 08\n",
      "finished: 2016 Jan 15\n",
      "finished: 2016 Jan 22\n",
      "finished: 2016 Jan 29\n",
      "finished: 2016 Feb 05\n",
      "finished: 2016 Feb 12\n",
      "finished: 2016 Feb 19\n",
      "finished: 2016 Feb 26\n",
      "finished: 2016 Mar 04\n",
      "finished: 2016 Mar 11\n",
      "finished: 2016 Mar 18\n",
      "finished: 2016 Mar 25\n",
      "finished: 2016 Apr 01\n",
      "finished: 2016 Apr 08\n",
      "finished: 2016 Apr 15\n",
      "finished: 2016 Apr 22\n",
      "finished: 2016 Apr 29\n",
      "finished: 2016 May 06\n",
      "finished: 2016 May 13\n",
      "finished: 2016 May 20\n",
      "finished: 2016 May 27\n",
      "finished: 2016 Jun 03\n",
      "finished: 2016 Jun 10\n",
      "finished: 2016 Jun 17\n",
      "finished: 2016 Jun 24\n",
      "finished: 2016 Jul 01\n",
      "finished: 2016 Jul 08\n",
      "finished: 2016 Jul 15\n",
      "finished: 2016 Jul 22\n",
      "finished: 2016 Jul 29\n",
      "finished: 2016 Aug 05\n",
      "finished: 2016 Aug 12\n",
      "finished: 2016 Aug 19\n",
      "finished: 2016 Aug 26\n",
      "finished: 2016 Sep 02\n",
      "finished: 2016 Sep 09\n",
      "finished: 2016 Sep 16\n",
      "finished: 2016 Sep 23\n",
      "finished: 2016 Sep 30\n",
      "finished: 2016 Oct 07\n",
      "finished: 2016 Oct 14\n",
      "finished: 2016 Oct 21\n",
      "finished: 2016 Oct 28\n",
      "finished: 2016 Nov 04\n",
      "finished: 2016 Nov 11\n",
      "finished: 2016 Nov 18\n",
      "finished: 2016 Nov 25\n",
      "finished: 2016 Dec 02\n",
      "finished: 2016 Dec 09\n",
      "finished: 2016 Dec 16\n",
      "finished: 2016 Dec 23\n"
     ]
    }
   ],
   "source": [
    "urls = scrapingURL(range1=2016,range2=2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scraping(basic_url,urlList,year,typeList=['Editorial','Research Article','Review Article','Letter','Book Review']):\n",
    "    #text data\n",
    "    textList = {}\n",
    "    for week in urlList[year]:\n",
    "        #classified by weeks\n",
    "        textList[week] = []\n",
    "        for i in range(2):\n",
    "            #read url\n",
    "            content = urlList[year][week][i]\n",
    "            b1 = urlopen(basic_url + content)\n",
    "            bs1bj = BeautifulSoup(b1, 'html5lib')\n",
    "            # check if there is article categories\n",
    "            # Since there are many redundant information\n",
    "            try:\n",
    "                articleType = bs1bj.find('meta', {'name': 'citation_article_type'}).get('content')\n",
    "            except:\n",
    "                articleType = 'none'\n",
    "            # while the type of paper is in the list( We only scrap several specific types of articles)\n",
    "            # 'Editorial','Research Article','Review Article','Letter','Book Review'\n",
    "            if articleType in typeList:\n",
    "                \n",
    "                textList[week].append(analyzeWebpage(bs1bj))             \n",
    "                time.sleep(2 * (1+random.random()) )\n",
    "        print('finished:' + week)\n",
    "    return textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyzeWebpage(bs1bj):\n",
    "    #title\n",
    "    title = bs1bj.find('meta', {'name': 'citation_title'}).get('content')\n",
    "\n",
    "    #Author\n",
    "    authorList = bs1bj.findAll('meta', {'name': 'citation_author'})\n",
    "    author = []\n",
    "    for m in authorList:\n",
    "        author.append(m.get('content'))\n",
    "    \n",
    "    #Article type\n",
    "    articleType = bs1bj.find('meta', {'name': 'citation_article_type'}).get('content')\n",
    "\n",
    "    #Institution\n",
    "    institutionList = bs1bj.findAll('meta', {'name': 'citation_author_institution'})\n",
    "    institution = []\n",
    "    for InstiData in institutionList:\n",
    "        institution.append(InstiData.get('content'))\n",
    "\n",
    "    #Publisher\n",
    "    publisher = bs1bj.find('meta', {'name': 'citation_publisher'}).get('content')\n",
    "\n",
    "    #Publication data\n",
    "    pubDate = bs1bj.find('meta', {'name': 'citation_publication_date'}).get('content')\n",
    "\n",
    "    #DOI\n",
    "    doi = bs1bj.find('meta', {'name': 'citation_doi'}).get('content')\n",
    "\n",
    "    ##PDF (need to be verified)\n",
    "    pdfLink = bs1bj.find('link', {'title': 'Full Text (PDF)'}).get('href')\n",
    "\n",
    "    ##text (need to be verified)\n",
    "    textLink = bs1bj.find('link', {'title': 'Full Text (Plain)'}).get('href')\n",
    "\n",
    "    # put them in to directory\n",
    "    textDict = {}\n",
    "    textDict['type'] = articleType\n",
    "    textDict['title'] = title\n",
    "    textDict['author'] = author\n",
    "    textDict['authorInstitution'] = institution\n",
    "    textDict['publisher'] = publisher\n",
    "    textDict['date'] = pubDate\n",
    "    textDict['doi'] = doi\n",
    "    textDict['pdfLink'] = pdfLink\n",
    "    textDict['textLink'] = textLink\n",
    "    # content\n",
    "    textDict['text'] = ''\n",
    "    textDict['abstract'] = ''\n",
    "    textDict['introduction'] = ''\n",
    "    textDict['discussion'] = ''\n",
    "\n",
    "    # Data for function to search by abstrace, introduction, discussion part\n",
    "    textParts = bs1bj.findAll('p', {'id': re.compile('p\\-[0-9]*')})\n",
    "    for finding in textParts:\n",
    "        textDict['text'] += finding.get_text().replace('\\n',' ') + ' '\n",
    "    # Abstract\n",
    "    # use try/except since articles may not have abstract\n",
    "    try:\n",
    "        abstractParts = bs1bj.findAll('div', {'class': 'section abstract'})\n",
    "        for finding in abstractParts:\n",
    "            textDict['abstract'] += finding.get_text().replace('\\n',' ') + ' '\n",
    "    except:\n",
    "        textDict['abstract'] = None\n",
    "\n",
    "    # Introduction\n",
    "    try:\n",
    "        introParts = bs1bj.findAll('div', {'class': 'section introduction'})\n",
    "        for finding in introParts:\n",
    "            textDict['introduction'] +=finding.get_text().replace('\\n',' ') + ' '\n",
    "    except:\n",
    "        textDict['introduction'] = None\n",
    "\n",
    "    # Discussion\n",
    "    try:\n",
    "        discussionParts = bs1bj.findAll('div', {'class': 'section discussion'})\n",
    "        for finding in discussionParts:\n",
    "            textDict['discussion'] +=  finding.get_text().replace('\\n',' ') + ' '\n",
    "    except:\n",
    "        textDict['discussion'] = None\n",
    "    \n",
    "    time.sleep(10*(1+random.random()))\n",
    "\n",
    "    return textDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveasJSON(textList,year):\n",
    "    model= textList #数据\n",
    "    with open(\"../data/json/test.json\",'w',encoding='utf-8') as json_file:\n",
    "        maximumIndex = json_file['maximumIndex']\n",
    "        \n",
    "    with open(\"../data/json/test/data.json\",'w',encoding='utf-8') as json_file:\n",
    "        maximumIndex = json_file['maximumIndex']\n",
    "        for week in model:\n",
    "            jsonDataList = model[week]\n",
    "            for d in jsonDataList:\n",
    "                d\n",
    "                json.dump(d,json_file,ensure_ascii=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished:Jan 01\n",
      "finished:Jan 08\n",
      "finished:Jan 15\n",
      "finished:Jan 22\n",
      "finished:Jan 29\n",
      "finished:Feb 05\n",
      "finished:Feb 12\n",
      "finished:Feb 19\n",
      "finished:Feb 26\n",
      "finished:Mar 04\n",
      "finished:Mar 11\n",
      "finished:Mar 18\n",
      "finished:Mar 25\n",
      "finished:Apr 01\n",
      "finished:Apr 08\n",
      "finished:Apr 15\n",
      "finished:Apr 22\n",
      "finished:Apr 29\n",
      "finished:May 06\n",
      "finished:May 13\n",
      "finished:May 20\n",
      "finished:May 27\n",
      "finished:Jun 03\n",
      "finished:Jun 10\n",
      "finished:Jun 17\n",
      "finished:Jun 24\n",
      "finished:Jul 01\n",
      "finished:Jul 08\n",
      "finished:Jul 15\n",
      "finished:Jul 22\n",
      "finished:Jul 29\n",
      "finished:Aug 05\n",
      "finished:Aug 12\n",
      "finished:Aug 19\n",
      "finished:Aug 26\n",
      "finished:Sep 02\n",
      "finished:Sep 09\n",
      "finished:Sep 16\n",
      "finished:Sep 23\n",
      "finished:Sep 30\n",
      "finished:Oct 07\n",
      "finished:Oct 14\n",
      "finished:Oct 21\n",
      "finished:Oct 28\n",
      "finished:Nov 04\n",
      "finished:Nov 11\n",
      "finished:Nov 18\n",
      "finished:Nov 25\n",
      "finished:Dec 02\n",
      "finished:Dec 09\n",
      "finished:Dec 16\n",
      "finished:Dec 23\n"
     ]
    }
   ],
   "source": [
    "textList = scraping(basic_url,urls,2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model= {'maximumIndex':0, 'data':{}} #数据\n",
    "with open(\"../data/json/test.json\",'w',encoding='utf-8') as json_file:\n",
    "    json.dump(model,json_file,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#textList['Jan 06'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p= {'maximumIndex':-1, 'data':{}} #数据\n",
    "\n",
    "\n",
    "Index = p['maximumIndex']\n",
    "for week in textList:\n",
    "    for d in textList[week]:\n",
    "        Index += 1\n",
    "        p['data'][Index] = d\n",
    "p['maximumIndex'] = Index\n",
    "\n",
    "with open(\"../data/json/test.json\",'w',encoding='utf-8') as json_file:\n",
    "    json.dump(p,json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../data/json/test.json\",'r',encoding='utf-8') as json_file:\n",
    "    p = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in p['data'].keys():\n",
    "    urlretrieve(basic_url+p['data'][key]['pdfLink'], '../data/pdf/' + str(key)+'.pdf')\n",
    "    time.sleep(30*(1 + random.random()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marcia McNutt']\n",
      "['David King']\n",
      "['Bruce Alberts']\n",
      "['Johan Rockström']\n",
      "['Geraldine Richmond']\n",
      "['Rush Holt']\n",
      "['Marcia McNutt']\n",
      "['Marcia McNutt']\n",
      "['Byung Gwon Lee']\n",
      "['Marcia McNutt']\n",
      "['Michael S. Turner']\n",
      "['S. J. Gates']\n",
      "['Paul G. Allen']\n",
      "['Anne Glover']\n",
      "['Marcia McNutt']\n",
      "['Julia K. Goodrich', 'Emily R. Davenport', 'Jillian L. Waters', 'Andrew G. Clark', 'Ruth E. Ley']\n",
      "['Alan I. Leshner']\n",
      "['Marcelo Sánchez Sorondo', 'Veerabhadran Ramanathan']\n",
      "['Marcia McNutt']\n",
      "['Marcia McNutt']\n",
      "['Guang-Zhong Yang', 'Marcia McNutt']\n",
      "['Wendy V. Gilbert', 'Tristan A. Bell', 'Cassandra Schaening']\n",
      "['Marcia McNutt']\n",
      "['Graeme Reid']\n",
      "['Jeremy Berg']\n",
      "['James Wilsdon']\n",
      "['France A. Córdova']\n",
      "['Jeremy Berg']\n",
      "['Helga Nowotny', 'Jana Kolar']\n",
      "['Jeremy Berg']\n",
      "['Peter Gluckman']\n",
      "['Michael T. Osterholm']\n",
      "['May R. Berenbaum']\n",
      "['David Baltimore']\n",
      "['Shaohua Fan', 'Matthew E. B. Hansen', 'Yancy Lo', 'Sarah A. Tishkoff']\n",
      "['Eric A. Miska', 'Anne C. Ferguson-Smith']\n",
      "['Subra Suresh', 'Robert A. Bradway']\n",
      "['Jeremy Berg']\n",
      "['Patricia Espinosa']\n",
      "['David L. Heymann']\n",
      "['Jeremy Berg']\n",
      "['Ernest J. Moniz']\n",
      "['Allan Goodman']\n",
      "['Jeremy Berg']\n"
     ]
    }
   ],
   "source": [
    "for key in p['data'].keys():\n",
    "    print(p['data'][key]['author'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
